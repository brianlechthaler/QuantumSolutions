{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Classification with QBoost\n",
    "This notebook uses QBoost, an algorithm developed all the way back in 2008 from a collaborative effort between Google and D-Wave. QBoost is used in this notebook to perform binary classification on two datasets, [MNIST (WARNING: No SSL!)](http://yann.lecun.com/exdb/mnist/), and [Breast Cancer Wisconsin (Original) Data Set](https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(original)) -- both from SciKit Learn's built-in datasets.\n",
    "\n",
    "This is certainly not a demonstration of any kind of quantum speedup, advantage, or whatever buzzword is being over-used. This binary classifier samples two very small datasets, a task that takes a trivial amount of time on most classical computers, smartphones, and probably even some IoT devices availible to consumers. That said, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS Braket-specific Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = 'braket-your-braket-bucket-check-s3-console-after-onboard'\n",
    "s3 = b, 'results'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runtime Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Sampler from AWS Braket 'braket' (default) or D-Wave Leap 'leap'\n",
    "sampler_vendor = 'braket'\n",
    "# Run The Experiment\n",
    "run_experiment = True\n",
    "# Test Classifier on MNIST Dataset\n",
    "mnist = True\n",
    "# Test Classifier on WISC Dataset\n",
    "wisc = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, AdaBoostRegressor\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from sklearn import preprocessing, metrics\n",
    "from sklearn.datasets import load_breast_cancer, fetch_openml\n",
    "from sklearn.impute import SimpleImputer\n",
    "from braket.ocean_plugin.braket_dwave_sampler import BraketDWaveSampler\n",
    "from dwave.system import DWaveSampler\n",
    "from dwave.system.composites import EmbeddingComposite\n",
    "import numpy as np\n",
    "from __future__ import print_function, division\n",
    "import sys\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Weight Penalty Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_penalty(prediction, y, percent = 0.1): \n",
    "    \"\"\"\n",
    "    For Regression we have to introduce a metric to penalize differences of the prediction from the label y.\n",
    "    Percent gives the maximum deviation of the prediction from the label that is not penalized.\n",
    "    \"\"\"\n",
    "    diff = np.abs(prediction-y)\n",
    "    min_ = diff.min()\n",
    "    max_ = diff.max()\n",
    "    norm = (diff-min_)/(max_-min_)\n",
    "    norm = 1.0*(norm  < percent)  \n",
    "    return norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Weak Classifiers Based on DecisionTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeakClassifiers(object):\n",
    "    \"\"\"\n",
    "    Weak Classifiers based on DecisionTree\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_estimators=50, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.estimators_ = []\n",
    "        self.max_depth = max_depth\n",
    "        self.__construct_wc()\n",
    "\n",
    "    def __construct_wc(self):\n",
    "\n",
    "        self.estimators_ = [DecisionTreeClassifier(max_depth=self.max_depth,\n",
    "                                                   random_state=np.random.randint(1000000,10000000))\n",
    "                            for _ in range(self.n_estimators)]\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        fit estimators\n",
    "        :param X:\n",
    "        :param y:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        self.estimator_weights = np.zeros(self.n_estimators)\n",
    "\n",
    "        d = np.ones(len(X)) / len(X)\n",
    "        for i, h in enumerate(self.estimators_):\n",
    "            h.fit(X, y, sample_weight=d)\n",
    "            pred = h.predict(X)\n",
    "            eps = d.dot(pred != y)\n",
    "            if eps == 0: # to prevent divided by zero error\n",
    "                eps = 1e-20\n",
    "            w = (np.log(1 - eps) - np.log(eps)) / 2\n",
    "            d = d * np.exp(- w * y * pred)\n",
    "            d = d / d.sum()\n",
    "            self.estimator_weights[i] = w\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        predict label of X\n",
    "        :param X:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        if not hasattr(self, 'estimator_weights'):\n",
    "            raise Exception('Not Fitted Error!')\n",
    "\n",
    "        y = np.zeros(len(X))\n",
    "\n",
    "        for (h, w) in zip(self.estimators_, self.estimator_weights):\n",
    "            y += w * h.predict(X)\n",
    "\n",
    "        y = np.sign(y)\n",
    "\n",
    "        return y\n",
    "\n",
    "    def copy(self):\n",
    "\n",
    "        classifier = WeakClassifiers(n_estimators=self.n_estimators, max_depth=self.max_depth)\n",
    "        classifier.estimators_ = deepcopy(self.estimators_)\n",
    "        if hasattr(self, 'estimator_weights'):\n",
    "            classifier.estimator_weights = np.array(self.estimator_weights)\n",
    "\n",
    "        return classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define QBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QBoostClassifier(WeakClassifiers):\n",
    "    \"\"\"\n",
    "    Qboost Classifier\n",
    "    \"\"\"\n",
    "    def __init__(self, n_estimators=50, max_depth=3):\n",
    "        super(QBoostClassifier, self).__init__(n_estimators=n_estimators,\n",
    "                                              max_depth=max_depth)\n",
    "\n",
    "    def fit(self, X, y, sampler, lmd=0.2, **kwargs):\n",
    "\n",
    "        n_data = len(X)\n",
    "\n",
    "        # step 1: fit weak classifiers\n",
    "        super(QBoostClassifier, self).fit(X, y)\n",
    "\n",
    "        # step 2: create QUBO\n",
    "        hij = []\n",
    "        for h in self.estimators_:\n",
    "            hij.append(h.predict(X))\n",
    "\n",
    "        hij = np.array(hij)\n",
    "        # scale hij to [-1/N, 1/N]\n",
    "        hij = 1. * hij / self.n_estimators\n",
    "\n",
    "        ## Create QUBO\n",
    "        qii = n_data * 1. / (self.n_estimators ** 2) + lmd - 2 * np.dot(hij, y)\n",
    "        qij = np.dot(hij, hij.T)\n",
    "        Q = dict()\n",
    "        Q.update(dict(((k, k), v) for (k, v) in enumerate(qii)))\n",
    "        for i in range(self.n_estimators):\n",
    "            for j in range(i + 1, self.n_estimators):\n",
    "                Q[(i, j)] = qij[i, j]\n",
    "\n",
    "        # step 3: optimize QUBO\n",
    "        res = sampler.sample_qubo(Q, **kwargs)\n",
    "        samples = np.array([[samp[k] for k in range(self.n_estimators)] for samp in res])\n",
    "\n",
    "        # take the optimal solution as estimator weights\n",
    "        self.estimator_weights = samples[0]\n",
    "\n",
    "    def predict(self, X):\n",
    "        n_data = len(X)\n",
    "        pred_all = np.array([h.predict(X) for h in self.estimators_])\n",
    "        temp1 = np.dot(self.estimator_weights, pred_all)\n",
    "        T1 = np.sum(temp1, axis=0) / (n_data * self.n_estimators * 1.)\n",
    "        y = np.sign(temp1 - T1) #binary classes are either 1 or -1\n",
    "\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Weak Regressor based on DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeakRegressor(object):\n",
    "    \"\"\"\n",
    "    Weak Regressor based on DecisionTreeRegressor\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_estimators=50, max_depth=3, DT = True, Ada = False, ):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.estimators_ = []\n",
    "        self.max_depth = max_depth\n",
    "        self.__construct_wc()\n",
    "\n",
    "    def __construct_wc(self):\n",
    "\n",
    "        self.estimators_ = [DecisionTreeRegressor(max_depth=self.max_depth,\n",
    "                                                   random_state=np.random.randint(1000000,10000000))\n",
    "                            for _ in range(self.n_estimators)]\n",
    "#        self.estimators_ = [AdaBoostRegressor(random_state=np.random.randint(1000000,10000000))\n",
    "#                            for _ in range(self.n_estimators)]\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        fit estimators\n",
    "        :param X:\n",
    "        :param y:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        self.estimator_weights = np.zeros(self.n_estimators) #initialize all estimator weights to zero\n",
    "\n",
    "        d = np.ones(len(X)) / len(X)\n",
    "        for i, h in enumerate(self.estimators_): #fit all estimators\n",
    "            h.fit(X, y, sample_weight=d)\n",
    "            pred = h.predict(X)\n",
    "            # For classification one simply compares (pred != y)\n",
    "            # For regression we have to define another metric\n",
    "            norm = weight_penalty(pred, y)\n",
    "            eps = d.dot(norm)\n",
    "            if eps == 0: # to prevent divided by zero error\n",
    "                eps = 1e-20\n",
    "            w = (np.log(1 - eps) - np.log(eps)) / 2\n",
    "            d = d * np.exp(- w * y * pred)\n",
    "            d = d / d.sum()\n",
    "            self.estimator_weights[i] = w\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        predict label of X\n",
    "        :param X:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        if not hasattr(self, 'estimator_weights'):\n",
    "            raise Exception('Not Fitted Error!')\n",
    "\n",
    "        y = np.zeros(len(X))\n",
    "\n",
    "        for (h, w) in zip(self.estimators_, self.estimator_weights):\n",
    "            y += w * h.predict(X)\n",
    "\n",
    "        y = np.sign(y)\n",
    "\n",
    "        return y\n",
    "\n",
    "    def copy(self):\n",
    "\n",
    "        classifier = WeakRegressor(n_estimators=self.n_estimators, max_depth=self.max_depth)\n",
    "        classifier.estimators_ = deepcopy(self.estimators_)\n",
    "        if hasattr(self, 'estimator_weights'):\n",
    "            classifier.estimator_weights = np.array(self.estimator_weights)\n",
    "\n",
    "        return classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define QBoost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QBoostRegressor(WeakRegressor):\n",
    "    \"\"\"\n",
    "    Qboost Regressor\n",
    "    \"\"\"\n",
    "    def __init__(self, n_estimators=50, max_depth=3):\n",
    "        super(QBoostRegressor, self).__init__(n_estimators=n_estimators,\n",
    "                                              max_depth=max_depth)\n",
    "        self.Qu = 0.0\n",
    "        self.hij = 0.0\n",
    "        self.var1 = 0.0\n",
    "        self.qij = 0.0\n",
    "\n",
    "    def fit(self, X, y, sampler, lmd=0.2, **kwargs):\n",
    "\n",
    "        n_data = len(X)\n",
    "\n",
    "        # step 1: fit weak classifiers\n",
    "        super(QBoostRegressor, self).fit(X, y)\n",
    "\n",
    "        # step 2: create QUBO\n",
    "        hij = []\n",
    "        for h in self.estimators_:\n",
    "            hij.append(h.predict(X))\n",
    "\n",
    "        hij = np.array(hij)\n",
    "        # scale hij to [-1/N, 1/N]\n",
    "        hij = 1. * hij / self.n_estimators\n",
    "        self.hij = hij\n",
    "        ## Create QUBO\n",
    "        qii = n_data * 1. / (self.n_estimators ** 2) + lmd - 2 * np.dot(hij, y)\n",
    "        self.var1 = qii\n",
    "        qij = np.dot(hij, hij.T)\n",
    "        self.qij = qij\n",
    "        Q = dict()\n",
    "        Q.update(dict(((k, k), v) for (k, v) in enumerate(qii)))\n",
    "        for i in range(self.n_estimators):\n",
    "            for j in range(i + 1, self.n_estimators):\n",
    "                Q[(i, j)] = qij[i, j]\n",
    "\n",
    "        self.Qu = Q\n",
    "        # step 3: optimize QUBO\n",
    "        res = sampler.sample_qubo(Q, **kwargs)\n",
    "        samples = np.array([[samp[k] for k in range(self.n_estimators)] for samp in res])\n",
    "\n",
    "        # take the optimal solution as estimator weights\n",
    "        # self.estimator_weights = np.mean(samples, axis=0)\n",
    "        self.estimator_weights = samples[0]\n",
    "\n",
    "    def predict(self, X):\n",
    "        n_data = len(X)\n",
    "        pred_all = np.array([h.predict(X) for h in self.estimators_])\n",
    "        temp1 = np.dot(self.estimator_weights, pred_all)\n",
    "        norm = np.sum(self.estimator_weights)\n",
    "        if norm > 0:\n",
    "            y = temp1 / norm\n",
    "        else:\n",
    "            y = temp1\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Function to Quantum Boost Existing Weak Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QboostPlus(object):\n",
    "    \"\"\"\n",
    "    Only for Classifiers\n",
    "    Quantum boost existing (weak) classifiers\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, weak_classifier_list):\n",
    "        self.estimators_ = weak_classifier_list\n",
    "        self.n_estimators = len(self.estimators_)\n",
    "        self.estimator_weights = np.ones(self.n_estimators) #estimator weights will be binary (Dwave output)\n",
    "\n",
    "    def fit(self, X, y, sampler, lmd=0.2, **kwargs):\n",
    "\n",
    "        n_data = len(X)\n",
    "        # step 1: create QUBO\n",
    "        hij = []\n",
    "        for h in self.estimators_:\n",
    "            hij.append(h.predict(X))\n",
    "\n",
    "        hij = np.array(hij)\n",
    "        # scale hij to [-1/N, 1/N]\n",
    "        hij = 1. * hij / self.n_estimators\n",
    "\n",
    "        ## Create QUBO\n",
    "        qii = n_data * 1. / (self.n_estimators ** 2) + lmd - 2 * np.dot(hij, y)\n",
    "        qij = np.dot(hij, hij.T)\n",
    "        Q = dict()\n",
    "        Q.update(dict(((k, k), v) for (k, v) in enumerate(qii)))\n",
    "        for i in range(self.n_estimators):\n",
    "            for j in range(i + 1, self.n_estimators):\n",
    "                Q[(i, j)] = qij[i, j]\n",
    "\n",
    "        # step 3: optimize QUBO\n",
    "        res = sampler.sample_qubo(Q, **kwargs)\n",
    "        samples = np.array([[samp[k] for k in range(self.n_estimators)] for samp in res])\n",
    "\n",
    "        # take the optimal solution as estimator weights\n",
    "        self.estimator_weights = samples[0]\n",
    "\n",
    "    def predict(self, X):\n",
    "\n",
    "        n_data = len(X)\n",
    "        T = 0\n",
    "        y = np.zeros(n_data)\n",
    "        for i, h in enumerate(self.estimators_):\n",
    "            y0 = self.estimator_weights[i] * h.predict(X)  # prediction of weak classifier\n",
    "            y += y0\n",
    "            T += np.sum(y0)\n",
    "\n",
    "        y = np.sign(y - T / (n_data*self.n_estimators))\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Function to Quantum Boost Weak Regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QboostPlusRegression(object):\n",
    "    \"\"\"\n",
    "    Quantum boost existing (weak) regressors\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, weak_Regressor_list):\n",
    "        self.estimators_ = weak_Regressor_list\n",
    "        self.n_estimators = len(self.estimators_)\n",
    "        self.estimator_weights = np.ones(self.n_estimators)\n",
    "\n",
    "    def fit(self, X, y, sampler, lmd=0.2, **kwargs):\n",
    "\n",
    "        n_data = len(X)\n",
    "        # step 1: create QUBO\n",
    "        hij = []\n",
    "        for h in self.estimators_:\n",
    "            hij.append(h.predict(X))\n",
    "\n",
    "        hij = np.array(hij)\n",
    "        # scale hij to [-1/N, 1/N]\n",
    "        hij = 1. * hij / self.n_estimators\n",
    "\n",
    "        ## Create QUBO\n",
    "        qii = n_data * 1. / (self.n_estimators ** 2) + lmd - 2 * np.dot(hij, y)\n",
    "        qij = np.dot(hij, hij.T)\n",
    "        Q = dict()\n",
    "        Q.update(dict(((k, k), v) for (k, v) in enumerate(qii)))\n",
    "        for i in range(self.n_estimators):\n",
    "            for j in range(i + 1, self.n_estimators):\n",
    "                Q[(i, j)] = qij[i, j]\n",
    "\n",
    "        # step 3: optimize QUBO\n",
    "        res = sampler.sample_qubo(Q, **kwargs)\n",
    "        samples = np.array([[samp[k] for k in range(self.n_estimators)] for samp in res])\n",
    "\n",
    "        # take the optimal solution as estimator weights\n",
    "        self.estimator_weights = samples[0]\n",
    "\n",
    "    def predict(self, X):\n",
    "\n",
    "        n_data = len(X)\n",
    "        T = 0\n",
    "        y = np.zeros(n_data)\n",
    "        for i, h in enumerate(self.estimators_):\n",
    "            y0 = self.estimator_weights[i] * h.predict(X)  # prediction of weak classifier\n",
    "            y += y0\n",
    "            T += np.sum(y0)\n",
    "\n",
    "        norm = np.sum(self.estimator_weights)\n",
    "        if norm > 0:\n",
    "            y = y / norm\n",
    "        else:\n",
    "            y = y\n",
    "\n",
    "        return y\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Function for Calculating Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric(y, y_pred):\n",
    "\n",
    "    return metrics.accuracy_score(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Function for Training a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X_train, y_train, X_test, y_test, lmd, sampler_vendor):\n",
    "    \"\"\"\n",
    "    Train qboost model\n",
    "\n",
    "    :param X_train: train input\n",
    "    :param y_train: train label\n",
    "    :param X_test: test input\n",
    "    :param y_test: test label\n",
    "    :param lmd: lmbda to control regularization term\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    NUM_READS = 3000\n",
    "    NUM_WEAK_CLASSIFIERS = 35\n",
    "    # lmd = 0.5\n",
    "    TREE_DEPTH = 3\n",
    "    \n",
    "    # define sampler\n",
    "    if sampler_vendor == 'braket':\n",
    "        dwave_sampler = BraketDWaveSampler(s3)\n",
    "    elif sampler_vendor == 'leap':\n",
    "        dwave_sampler = DWaveSampler()\n",
    "    elif sampler_vendor == None:\n",
    "        print('ERROR: Sampler not specified.')\n",
    "        dwave_sampler = None\n",
    "    else:\n",
    "        print('ERROR: Unable to create sampler, invalid sampler specified.')\n",
    "        \n",
    "    # embed sampler using EmbeddingComposite\n",
    "    emb_sampler = EmbeddingComposite(dwave_sampler)\n",
    "\n",
    "    N_train = len(X_train)\n",
    "    N_test = len(X_test)\n",
    "\n",
    "    print(\"\\n======================================\")\n",
    "    print(\"Train#: %d, Test: %d\" %(N_train, N_test))\n",
    "    print('Num weak classifiers:', NUM_WEAK_CLASSIFIERS)\n",
    "    print('Tree depth:', TREE_DEPTH)\n",
    "\n",
    "\n",
    "    # input: dataset X and labels y (in {+1, -1}\n",
    "\n",
    "    # Preprocessing data\n",
    "    # imputer = SimpleImputer()\n",
    "    scaler = preprocessing.StandardScaler()     # standardize features\n",
    "    normalizer = preprocessing.Normalizer()     # normalize samples\n",
    "\n",
    "    # X = imputer.fit_transform(X)\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_train = normalizer.fit_transform(X_train)\n",
    "\n",
    "    # X_test = imputer.fit_transform(X_test)\n",
    "    X_test = scaler.fit_transform(X_test)\n",
    "    X_test = normalizer.fit_transform(X_test)\n",
    "\n",
    "    ## Adaboost\n",
    "    print('\\nAdaboost')\n",
    "\n",
    "    clf = AdaBoostClassifier(n_estimators=NUM_WEAK_CLASSIFIERS)\n",
    "\n",
    "    # scores = cross_val_score(clf, X, y, cv=5, scoring='accuracy')\n",
    "    print('fitting...')\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    hypotheses_ada = clf.estimators_\n",
    "    # clf.estimator_weights_ = np.random.uniform(0,1,size=NUM_WEAK_CLASSIFIERS)\n",
    "    print('testing...')\n",
    "    y_train_pred = clf.predict(X_train)\n",
    "    y_test_pred = clf.predict(X_test)\n",
    "\n",
    "    print('accu (train): %5.2f'%(metric(y_train, y_train_pred)))\n",
    "    print('accu (test): %5.2f'%(metric(y_test, y_test_pred)))\n",
    "\n",
    "    # Ensembles of Decision Tree\n",
    "    print('\\nDecision tree')\n",
    "\n",
    "    clf2 = WeakClassifiers(n_estimators=NUM_WEAK_CLASSIFIERS, max_depth=TREE_DEPTH)\n",
    "    clf2.fit(X_train, y_train)\n",
    "\n",
    "    y_train_pred2 = clf2.predict(X_train)\n",
    "    y_test_pred2 = clf2.predict(X_test)\n",
    "    print(clf2.estimator_weights)\n",
    "\n",
    "    print('accu (train): %5.2f' % (metric(y_train, y_train_pred2)))\n",
    "    print('accu (test): %5.2f' % (metric(y_test, y_test_pred2)))\n",
    "\n",
    "    # Ensembles of Decision Tree\n",
    "    print('\\nQBoost')\n",
    "\n",
    "    DW_PARAMS = {'num_reads': NUM_READS,\n",
    "                 'auto_scale': True,\n",
    "                 # \"answer_mode\": \"histogram\",\n",
    "                 'num_spin_reversal_transforms': 10,\n",
    "                 # 'annealing_time': 10,\n",
    "                 }\n",
    "\n",
    "    clf3 = QBoostClassifier(n_estimators=NUM_WEAK_CLASSIFIERS, max_depth=TREE_DEPTH)\n",
    "    clf3.fit(X_train, y_train, emb_sampler, lmd=lmd, **DW_PARAMS)\n",
    "\n",
    "    y_train_dw = clf3.predict(X_train)\n",
    "    y_test_dw = clf3.predict(X_test)\n",
    "\n",
    "    print(clf3.estimator_weights)\n",
    "\n",
    "    print('accu (train): %5.2f' % (metric(y_train, y_train_dw)))\n",
    "    print('accu (test): %5.2f' % (metric(y_test, y_test_dw)))\n",
    "\n",
    "\n",
    "    # Ensembles of Decision Tree\n",
    "    print('\\nQBoostPlus')\n",
    "    clf4 = QboostPlus([clf, clf2, clf3])\n",
    "    clf4.fit(X_train, y_train, emb_sampler, lmd=lmd, **DW_PARAMS)\n",
    "    y_train4 = clf4.predict(X_train)\n",
    "    y_test4 = clf4.predict(X_test)\n",
    "    print(clf4.estimator_weights)\n",
    "\n",
    "    print('accu (train): %5.2f' % (metric(y_train, y_train4)))\n",
    "    print('accu (test): %5.2f' % (metric(y_test, y_test4)))\n",
    "\n",
    "\n",
    "    print(\"=============================================\")\n",
    "    print(\"Method \\t Adaboost \\t DecisionTree \\t Qboost \\t QboostIt\")\n",
    "    print(\"Train\\t %5.2f \\t\\t %5.2f \\t\\t\\t %5.2f \\t\\t %5.2f\"% (metric(y_train, y_train_pred),\n",
    "                                                               metric(y_train, y_train_pred2),\n",
    "                                                               metric(y_train, y_train_dw),\n",
    "                                                               metric(y_train, y_train4)))\n",
    "    print(\"Test\\t %5.2f \\t\\t %5.2f \\t\\t\\t %5.2f \\t\\t %5.2f\"% (metric(y_test, y_test_pred),\n",
    "                                                              metric(y_test,y_test_pred2),\n",
    "                                                              metric(y_test, y_test_dw),\n",
    "                                                              metric(y_test, y_test4)))\n",
    "    print(\"=============================================\")\n",
    "\n",
    "    plt.subplot(211)\n",
    "    plt.bar(range(len(y_test)), y_test)\n",
    "    plt.subplot(212)\n",
    "    plt.bar(range(len(y_test)), y_test_dw)\n",
    "    plt.show()\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Experiment!\n",
    "This tests our classifier on two datasets, first [MNIST (WARNING: No SSL!)](http://yann.lecun.com/exdb/mnist/), then [Breast Cancer Wisconsin (Original) Data Set](https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(original)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================\n",
      "Train#: 3333, Test: 1667\n",
      "Num weak classifiers: 35\n",
      "Tree depth: 3\n",
      "\n",
      "Adaboost\n",
      "fitting...\n",
      "testing...\n",
      "accu (train):  0.86\n",
      "accu (test):  0.83\n",
      "\n",
      "Decision tree\n",
      "[0.58204807 0.49024972 0.38958053 0.42547385 0.26738606 0.3189999\n",
      " 0.34184563 0.27676042 0.37951279 0.21577486 0.33583947 0.34912615\n",
      " 0.37422525 0.31215134 0.29590478 0.27917402 0.28313412 0.18158317\n",
      " 0.31690535 0.2602812  0.22597227 0.24789463 0.16682482 0.3179388\n",
      " 0.23042279 0.39515907 0.37067034 0.20169936 0.28886562 0.43300414\n",
      " 0.4352297  0.31234166 0.23901466 0.39598776 0.28330392]\n",
      "accu (train):  0.98\n",
      "accu (test):  0.89\n",
      "\n",
      "QBoost\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "accu (train):  0.98\n",
      "accu (test):  0.90\n",
      "\n",
      "QBoostPlus\n",
      "[1 1 1]\n",
      "accu (train):  0.98\n",
      "accu (test):  0.89\n",
      "=============================================\n",
      "Method \t Adaboost \t DecisionTree \t Qboost \t QboostIt\n",
      "Train\t  0.86 \t\t  0.98 \t\t\t  0.98 \t\t  0.98\n",
      "Test\t  0.83 \t\t  0.89 \t\t\t  0.90 \t\t  0.89\n",
      "=============================================\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================\n",
      "Train#: 379, Test: 190\n",
      "Num weak classifiers: 35\n",
      "Tree depth: 3\n",
      "\n",
      "Adaboost\n",
      "fitting...\n",
      "testing...\n",
      "accu (train):  1.00\n",
      "accu (test):  0.97\n",
      "\n",
      "Decision tree\n",
      "[1.80410578 1.59827985 1.93452176 1.7257916  1.51111906 1.84446929\n",
      " 1.38874752 1.37445901 1.62789563 1.38994524 1.5814096  2.43586949\n",
      " 1.98568216 1.46540255 1.65100799 1.31047453 1.52530683 1.7332396\n",
      " 1.42666629 1.61361036 1.2021996  1.06466341 1.5711889  1.15060123\n",
      " 1.62183222 1.10257354 1.14481147 0.98463299 1.48586854 1.52933254\n",
      " 1.37137464 1.22256277 1.42006973 1.21912093 1.51031667]\n",
      "accu (train):  1.00\n",
      "accu (test):  0.98\n",
      "\n",
      "QBoost\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "accu (train):  1.00\n",
      "accu (test):  0.98\n",
      "\n",
      "QBoostPlus\n",
      "[1 1 1]\n",
      "accu (train):  1.00\n",
      "accu (test):  0.98\n",
      "=============================================\n",
      "Method \t Adaboost \t DecisionTree \t Qboost \t QboostIt\n",
      "Train\t  1.00 \t\t  1.00 \t\t\t  1.00 \t\t  1.00\n",
      "Test\t  0.97 \t\t  0.98 \t\t\t  0.98 \t\t  0.98\n",
      "=============================================\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if run_experiment == True:\n",
    "\n",
    "    if mnist == True:\n",
    "\n",
    "        mnist = fetch_openml('mnist_784', version=1)\n",
    "\n",
    "        idx = np.arange(len(mnist['data']))\n",
    "        np.random.shuffle(idx)\n",
    "\n",
    "        n = 5000\n",
    "        idx = idx[:n]\n",
    "        idx_train = idx[:2*n//3]\n",
    "        idx_test = idx[2*n//3:]\n",
    "\n",
    "        X_train = mnist['data'][idx_train]\n",
    "        X_test = mnist['data'][idx_test]\n",
    "\n",
    "        # Note: mnist['target'] is an array of string numbers, hence the comparison with '4'\n",
    "        y_train = 2*(mnist['target'][idx_train] <= '4') - 1\n",
    "        y_test = 2*(mnist['target'][idx_test] <= '4') - 1\n",
    "\n",
    "        clfs = train_model(X_train, y_train, X_test, y_test, 1.0, sampler_vendor)\n",
    "\n",
    "    if wisc == True:\n",
    "\n",
    "        wisc = load_breast_cancer()\n",
    "\n",
    "        idx = np.arange(len(wisc.target))\n",
    "        np.random.shuffle(idx)\n",
    "\n",
    "        # train on a random 2/3 and test on the remaining 1/3\n",
    "        idx_train = idx[:2*len(idx)//3]\n",
    "        idx_test = idx[2*len(idx)//3:]\n",
    "\n",
    "        X_train = wisc.data[idx_train]\n",
    "        X_test = wisc.data[idx_test]\n",
    "\n",
    "        y_train = 2 * wisc.target[idx_train] - 1  # binary -> spin\n",
    "        y_test = 2 * wisc.target[idx_test] - 1\n",
    "\n",
    "        clfs = train_model(X_train, y_train, X_test, y_test, 1.0, sampler_vendor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "H. Neven, V. S. Denchev, G. Rose, and W. G. Macready, \"Training a Binary\n",
    "Classifier with the Quantum Adiabatic Algorithm\",\n",
    "[arXiv:0811.0416v1](https://arxiv.org/pdf/0811.0416.pdf)\n",
    "\n",
    "Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. \"Gradient-based learning applied to document recognition.\" Proceedings of the IEEE, 86(11):2278-2324, November 1998. [doi:10.1109/5.726791](https://ieeexplore.ieee.org/document/726791)\n",
    "\n",
    "O. L. Mangasarian and W. H. Wolberg: \"Cancer diagnosis via linear programming\", SIAM News, Volume 23, Number 5, September 1990, pp 1 & 18.\n",
    "\n",
    "William H. Wolberg and O.L. Mangasarian: \"Multisurface method of pattern separation for medical diagnosis applied to breast cytology\", Proceedings of the National Academy of Sciences, U.S.A., Volume 87, December 1990, pp 9193-9196.\n",
    "\n",
    "O. L. Mangasarian, R. Setiono, and W.H. Wolberg: \"Pattern recognition via linear programming: Theory and application to medical diagnosis\", in: \"Large-scale numerical optimization\", Thomas F. Coleman and Yuying Li, editors, SIAM Publications, Philadelphia 1990, pp 22-30.\n",
    "\n",
    "K. P. Bennett & O. L. Mangasarian: \"Robust linear programming discrimination of two linearly inseparable sets\", Optimization Methods and Software 1, 1992, 23-34 (Gordon & Breach Science Publishers)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
